{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModLin(nn.Module):\n",
    "\n",
    "    def __init__(self, code_vec, W_mat, b_vec):\n",
    "        '''\n",
    "        code_vec = tensor [size_cond]\n",
    "        W_mat = tensor [size_out, size_in]\n",
    "        b_vec = tensor [size_out]\n",
    "        '''\n",
    "        super(ModLin, self).__init__()\n",
    "        self.code_vec = code_vec\n",
    "        self.W_mat = W_mat\n",
    "        self.b_vec = b_vec\n",
    "        self.w_c_fc = nn.Linear(self.code_vec.shape[-1], self.W_mat.shape[-1])\n",
    "        self.layernorm1 = nn.LayerNorm(self.W_mat.shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        temp = x\n",
    "        transformed_code = self.layernorm1(self.w_c_fc(self.code_vec))\n",
    "        temp = transformed_code * temp\n",
    "        temp = temp @ self.W_mat.T\n",
    "        temp = temp + self.b_vec\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, code_vec, W_mat, b_vec, num_layers=2):\n",
    "        '''\n",
    "        code_vec = tensor [size_cond]\n",
    "        W_mat = tensor [size_out, size_in]\n",
    "        b_vec = tensor [size_out]\n",
    "        '''\n",
    "        super(ModMLP, self).__init__()\n",
    "        self.code_vec = code_vec\n",
    "        self.W_mat = W_mat\n",
    "        self.b_vec = b_vec\n",
    "        self.w_c_fc = nn.Linear(self.code_vec.shape[-1], self.W_mat.shape[-1])\n",
    "        self.layernorm1 = nn.LayerNorm(self.W_mat.shape[-1])\n",
    "        self.mod_lin = ModLin(self.code_vec, self.W_mat, self.b_vec)\n",
    "        self.num_layers = num_layers\n",
    "        layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            layers.append(ModLin(self.code_vec, self.W_mat, self.b_vec))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModMultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, code, W, b, dropout):\n",
    "        super(ModMultiHeadAttentionLayer, self).__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = W.shape[0] // n_heads\n",
    "        self.code = code\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.fc_q = ModLin(self.code, self.W, self.b)\n",
    "        self.fc_k = ModLin(self.code, self.W, self.b)\n",
    "        self.fc_v = ModLin(self.code, self.W, self.b)\n",
    "        \n",
    "        self.fc_o = ModLin(self.code, self.W, self.b)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "        \n",
    "    def forward(self, query, key, value, compability_u, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        #compability_u = [batch, value len]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)    \n",
    "        \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "\n",
    "        c_u_i = compability_u.repeat(self.n_heads, attention.shape[2],1,1).permute(2,0,3,1)\n",
    "        ### [head, len, batch, len] -> [batch, head, len, len]\n",
    "        c_u_j = c_u_i.permute(0,1,3,2)\n",
    "\n",
    "\n",
    "        attention = c_u_i * c_u_j * attention\n",
    "        attention = attention / (1e-6 + attention)\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOC(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_dim, n_heads, code, W, b, num_mlp_layers, dropout):\n",
    "        super(LOC, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.code = code\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(hid_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(hid_dim)\n",
    "        self.mod_attn = ModMultiHeadAttentionLayer(hid_dim, n_heads, code, W, b, dropout)\n",
    "        self.mod_mlp = ModMLP(code, W, b, num_layers=num_mlp_layers)\n",
    "    \n",
    "    def forward(self, x, c_u):\n",
    "\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #c_u = [batch, len]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        #compability_u = [value len]\n",
    "        \n",
    "        temp = x \n",
    "        temp = self.layernorm1(temp)\n",
    "        a_u_hat = self.mod_attn(temp,temp,temp,c_u)[0]\n",
    "        # a_u_hat = [batch, len, hid]\n",
    "        a_u = a_u_hat * c_u.unsqueeze(-1).repeat_interleave(repeats=a_u_hat.shape[2], dim=-1) + x\n",
    "        b_hat_u = self.mod_mlp(self.layernorm2(a_u))\n",
    "        y_u = b_hat_u * c_u.unsqueeze(-1).repeat_interleave(repeats=a_u_hat.shape[2], dim=-1) + a_u\n",
    "        return y_u\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiniFunc(nn.Module):\n",
    "\n",
    "  def __init__(self, hid_dim, n_heads, type_size, code_size, num_locs, W, b, num_mlp_layers, dropout):\n",
    "    super(DiniFunc, self).__init__()\n",
    "    self.type_vec = nn.parameter.Parameter(F.normalize(torch.randn(type_size), dim=0), requires_grad=True)\n",
    "    self.code_vec = nn.parameter.Parameter(F.normalize(torch.randn(code_size), dim=0), requires_grad=True)\n",
    "    loc_factory_kwargs = {'hid_dim':hid_dim, 'n_heads':n_heads , 'code':self.code_vec,\\\n",
    "                              'W': W, 'b': b, 'num_mlp_layers': num_mlp_layers, 'dropout': dropout}\n",
    "    self.locs = nn.ModuleList([LOC(**loc_factory_kwargs) for i in range(num_locs)])\n",
    "\n",
    "    \n",
    "  def get_compatibility_score(self, type_x):\n",
    "    ### type_x = tensor [batch, len, type_size]\n",
    "    compatibility = type_x @ self.type_vec\n",
    "    ### [batch,len]\n",
    "    return compatibility\n",
    "\n",
    "  def forward(self, x, c_u):\n",
    "    temp = x \n",
    "    for loc in self.locs:\n",
    "      temp = temp + loc(temp, c_u)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiniFuncRow(nn.Module):\n",
    "\n",
    "  def __init__(self, hid_dim, n_heads, num_funcs, type_size, code_size, threshold, num_locs, num_mlp_layers, dropout):\n",
    "    super(DiniFuncRow, self).__init__()\n",
    "    self.W = nn.parameter.Parameter(torch.randn(hid_dim, hid_dim), requires_grad=True)\n",
    "    self.b =nn.parameter.Parameter(torch.randn(hid_dim), requires_grad=True)\n",
    "    self.threshold = threshold\n",
    "    func_factory_kwargs = {'hid_dim': hid_dim, 'n_heads': n_heads, 'type_size':type_size,\\\n",
    "       'code_size':code_size, 'num_locs': num_locs, 'W':self.W, 'b':self.b, 'num_mlp_layers':num_mlp_layers,\\\n",
    "        'dropout': dropout}\n",
    "    self.funcs = nn.ModuleList([DiniFunc(**func_factory_kwargs) for i in range(num_funcs)])\n",
    "    self.type_inference = nn.Sequential(nn.Linear(hid_dim, 2*hid_dim), nn.ReLU(), nn.Linear(2*hid_dim, type_size))\n",
    "    self.sigma = 10\n",
    "\n",
    "  def get_compability_matrix(self, x):\n",
    "    compability_list = []\n",
    "    types_x = self.type_inference(x) ## [batch, len, type_size]\n",
    "    for func in self.funcs:\n",
    "      compability = func.get_compatibility_score(types_x)\n",
    "      compability_list.append(compability)\n",
    "    compability_matrix = torch.stack(compability_list, dim=-1)\n",
    "    temp = compability_matrix\n",
    "    compability_matrix = torch.exp(-1*(1-temp)/self.sigma) * (compability_matrix > self.threshold)\n",
    "    compability_matrix = compability_matrix / (compability_matrix.sum(dim=2).unsqueeze(-1).repeat_interleave(repeats=compability_matrix.shape[2], dim=-1) + 1e-6)\n",
    "    return compability_matrix\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = [batch, len, size]\n",
    "    temp = x\n",
    "    compability_matrix = self.get_compability_matrix(x)\n",
    "    for i_func,func in enumerate(self.funcs):\n",
    "      temp = temp + func(x, compability_matrix[:,:,i_func]) * compability_matrix[:,:,i_func].unsqueeze(-1).repeat_interleave(repeats=temp.shape[2], dim=-1)\n",
    "    return temp\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiniFuncRowIter(nn.Module):\n",
    "\n",
    "  def __init__(self, hid_dim, n_heads, num_iter, num_funcs, type_size, code_size, threshold, num_locs, num_mlp_layers, dropout):\n",
    "    super(DiniFuncRowIter, self).__init__()\n",
    "    funcrow_factory_kwargs = {'hid_dim': hid_dim, 'n_heads': n_heads, 'num_funcs':num_funcs,\\\n",
    "       'type_size':type_size,'code_size':code_size, 'threshold':threshold, 'num_locs': num_locs,\\\n",
    "        'num_mlp_layers':num_mlp_layers, 'dropout': dropout}\n",
    "    self.funcs_row = DiniFuncRow(**funcrow_factory_kwargs)\n",
    "    self.num_iter = num_iter\n",
    "  \n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = [batch, len, size]\n",
    "    temp = x \n",
    "    for iter_num in range(self.num_iter):\n",
    "      temp = temp + self.funcs_row(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiniEncoder(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, num_layer, num_iter, num_funcs, type_size, code_size, threshold, num_locs, num_mlp_layers, dropout):\n",
    "        super(DiniEncoder, self).__init__()\n",
    "        \n",
    "        diniFuncRowIter_factory_kwargs = {'hid_dim': hid_dim, 'n_heads': n_heads, 'num_iter':num_iter,\\\n",
    "        'num_funcs':num_funcs,'type_size':type_size,'code_size':code_size, 'threshold':threshold,\\\n",
    "        'num_locs': num_locs,'num_mlp_layers':num_mlp_layers, 'dropout': dropout}\n",
    "\n",
    "        self.layers = nn.ModuleList([DiniFuncRowIter(**diniFuncRowIter_factory_kwargs) for i in range(num_layer)])\n",
    "\n",
    "    def forward(self, x):\n",
    "      temp = x\n",
    "      for layer in self.layers:\n",
    "        temp = temp + layer(temp)\n",
    "      return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-66.6636,  49.6996,  36.2375,  63.5610, -36.7596],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([-518.7077,  366.9874,  311.1058,  613.2228, -405.2093],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((64, 10, 128))\n",
    "W = torch.randn((128, 128))\n",
    "b = torch.randn(128)\n",
    "code = torch.randn(16)\n",
    "\n",
    "m = DiniEncoder(128, 8, 4, 3, 5, 16, 16, 0.01, 2, 1, 0.1)\n",
    "a = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((512, 128))\n",
    "W = torch.randn((128, 128))\n",
    "b = torch.randn(128)\n",
    "code = torch.randn(16)\n",
    "\n",
    "m = ModLin(code, W, b)\n",
    "m(x).shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73872a7625a445826f9d574d6c0b680d3fefaf18301176724424f6d82d4937fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
